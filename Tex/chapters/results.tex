\subsection{Technical Details}
    The generation of the Graphs and the Monte Carlo Simulation are implemented
    in C, all needed random numbers are generated by the GSL \cite{GSL}
    implementation of the \emph{Mersenne Twister} \cite{Matsumoto1998} and
    the generated data is evaluated via Python scripts.
    All source code is available at \url{https://github.com/surt91/IsingFerromagnet}.\\
    Most simulations were carried out on HERO, the \emph{H}igh-\emph{E}nd
    Computing \emph{R}esource \emph{O}ldenburg.\\

    For evaluation the Monte Carlo simulation is run until the system
    is equilibrated after \(t_{eq}\) sweeps. Then the simulation continues
    and the magnetization per site \(m=\frac{1}{N}\sum_i s_i\) and energy
    per site \(E=\frac{1}{N} H\) are calculated and saved for every
    \(2\tau\) sweeps. Where \(\tau\) denotes the \emph{autocorrelation time}
    (see also \ref{sssec:eqtime}). The used values for different system sizes
    are listed in tab.\ \ref{tab:tauAndTeq} in numbers of sweeps. Note that
    the \(t_{eq}\) values are generously rounded up to be on the safe side
    and the \(\tau\) values are determined as the maximum \(\tau\) over all
    simulated temperatures and disorder parameters \(\tau = \underset{T,\sigma}{\max} \{\tau_{T,\sigma}\}\)
    and rounded up to the next integer.
    \begin{table}[htbp]
        \center
        \begin{tabular}{l r r r r r}
            \toprule
            \(L\)    & 16 &  32 &  64 & 128 & 256\\
            \midrule
            \(\tau\) &  3 &   5 &   7 &  12 &  16\\
            \(t_{eq}\) & 40 & 100 & 100 & 200 & 600\\
            \bottomrule
        \end{tabular}
        \caption{Autocorrelation Times $\tau$ and Equilibration Times $t_{eq}$}
        \label{tab:tauAndTeq}
    \end{table}\\
    For every observable \(O\) the expected value \(\avg{O}\) is determined
    as the mean of \(N_{\mathrm{measure}}=10000\) measurements for \(L=16,32\)
    or \(N_{\mathrm{measure}}=5000\) for \(L=64,128\). The number of
    calculated sweeps totals to
    \[N_{\mathrm{sweeps}}=t_{eq}+2\tau N_{\mathrm{measure}}\]
    The expected values \(\avg{O}\) for \(100\) different random proximity
    graphs with the same disorder parameter \(\sigma\) are then averaged to
    \(\overline{\avg{O}}\). Note that the signs \(\overline{\avg{\cdot}}\)
    are omitted in the following for the sake of simplicity, so that the
    it will be just called \(O\).\\
    Not only the seed to generate the new random
    realization of the proximity graph gets changed, but also the seed for
    the random numbers used in the Monte Carlo simulation and during the
    generation of the random start configuration of the spins.\\
    The errors \(\Delta \overline{\avg{O}}\) are estimated by bootstrapping
    \cite{Bootstrap} with 200 bootstrap samples if not noted otherwise.
    Note that while \(\Delta \avg{O}\) is dependend on \(\tau\) \cite[p. 151]{Katzgraber2011}
    \(\Delta \overline{\avg{O}}\) is not directly, because it is averaged
    over different realizations of the random proximity graph, which are
    surely uncorrelated.
    Because every error mentioned in this thesis is of this type, it is not
    necessary to determine \(\tau\) for each observable. Therefore a good
    error estimate can be achieved by simple bootstrapping. Though one has
    to simulate enough uncorrelated states for each realization, to keep
    the \(\Delta \overline{\avg{O}}\) small. For every determined value an
    error is calculated and given in the form \texttt{value(error of last digit)}.
    The error given for fits are the asymptotic standard errors as calculated
    by gnuplot. Gnuplot is an open source plotting program, used for all plots
    and fits in this thesis \footnote{\url{http://gnuplot.info/}.}.

\subsection{Short Analysis of the Autocorrelation Time  $\tau$}
\label{ssec:results:autocorr}
    % Sollte das hier rein?
    Before the results are presented, a short analysis of the
    autocorrelation time \(\tau\) is given, to illustrate the benefits
    of the used Wolff cluster algorithm.\\
    In \ref{fig:autocorr}\subref{fig:autocorr:temperatures}
    the autocorrelation time for the magnetization per site \(m\) at
    \(\sigma=0\) is plotted. Note, that these are the unrounded values
    of tab.\ \ref{tab:tauAndTeq}
    \begin{figure}[htbp]
        \centering
        \subfigure[in Dependence on the Temperature $T$][]
        {
            \label{fig:autocorr:temperatures}
            \includegraphics[width=0.45\textwidth]{plots/autocorrT}
        }
        \subfigure[in Dependence on the System Size $L$][]
        {
            \label{fig:autocorr:powerLaw}
            \includegraphics[width=0.45\textwidth]{plots/autocorrL}
        }
        \caption[The Autocorrelation Time $\tau$]
        {
            Dependece of the autocorrelation time $\tau$ on
            \subref{fig:autocorr:temperatures} the temperature $T$ for
                $\sigma=0$ and
            \subref{fig:autocorr:powerLaw} the system size $L$ (errorbars
                are the standard error determined by bootstrapping)
        }
        \label{fig:autocorr}
    \end{figure}
    The plateau at low temperatures is easy to understand considering the
    effects of the Wolff algorithm. At low temperatures it flips nearly every
    spin in every step, thus the correlation drops to zero after one sweep.
    Alternatively the parallel tempering algorithm swaps spin configurations
    with possibly different signs, thus having the same effect.
    Also note that the maximum of \(\tau\) is not at \(T_c\) but at a higher
    temperature. The cause is probably the effectiveness of the Wolff cluster
    algorithm at \(T_c\), hence \(\tau \approx 1\) at \(T_{c}\).
    Obviously the autocorrelation time \(\tau\) increases with the system
    size. In fact it obeys a power law \(\tau \propto L^z\), which is
    the expected behavior of a dynamical exponent \(z\) as mentioned in
    section \ref{sssec:eqtime}.
    In fig.\ \ref{fig:autocorr}\subref{fig:autocorr:powerLaw} \(\tau\) is plotted
    over \(L\). Note that the exponent \(z'=0.64(2)\) determined by this plot is
    not comparable to the known critical exponent \(z=0.25\) \cite{NewmanBarkema1999}
    for the Wolff cluster algorithm. On the one hand the number of sweeps
    in which \(\tau\) is measured in this thesis are more than a standard
    Metropolis sweep to which \(z\) normally corresponds, because each sweep
    alongside the \(N\) Metropolis flips also a cluster of \(\ge 1\) spin is
    flipped. On the other hand the \(\tau\) used for the fit are not the
    \(\tau\) at \(T_{c}\) but the maximum \(\tau\) of all \(T\).
    Nevertheless it is interesting, that it obeys a power law
    \(\tau \propto L^{z'}\) just the same. This suggests, that \(\xi \gtrsim L\)
    is satisfied at the temperature of maximum \(\tau\), which is plausible,
    because the peak in fig.\ \ref{fig:autocorr}\subref{fig:autocorr:powerLaw}
    is near \(T_{c}\).\\
    Anyhow, fig.\ \ref{fig:autocorr}\subref{fig:autocorr:powerLaw} alone
    proofs the effectiveness of the Wolff cluster algorithm at criticality.\\
    This is of course only a rough analysis of existing data which was
    generated for another purpose. For a more in depth inspection, one
    would perform the simulation at the critical point, which will be
    determined in the next sections.

\subsection{Finite Size Effects}
\label{ssec:finitesize}
    The aim of this thesis is to find the critical temperature \(T_c\)
    of the disordered Ising model in dependence of the disorder parameter
    \(\sigma\). At \(T_c\) the mean absolute magnetization \(\avg{|m|}\) of
    the system will show a steep decline to zero and the susceptibility
    \(\chi = \frac{N}{T}\brac{\avg{m^2}-\avg{m}^2}\)
    will diverge. In fig.\ \ref{fig:smeared_out}\subref{sfig:smeared_out:meanM_L}
    it is easy to see that the steep decline of \(\avg{|m|}\)
    occurs at lower temperatures \(T\) for higher
    disorder parameters \(\sigma\) (here with a RNG).
    \begin{figure}[htbp]
        \centering
        \subfigure[Dependence of the Phase Transition on $\sigma$][]
        {
            \label{sfig:smeared_out:meanM_L}
            \includegraphics[width=0.45\textwidth]{plots/Mean_M_L_128}
        }
        \subfigure[Example for Finite Size Effects][]
        {
            \label{sfig:smeared_out:meanM}
            \includegraphics[width=0.45\textwidth]{plots/meanM}
        }
        \caption[Phase Transition and Finite Size Effects]
        {
            \subref{sfig:smeared_out:meanM_L}: Effects of the disorder
            parameter $\sigma$ on the phase transition
            for an underlying RNG with $L=128$.\\
            \subref{sfig:smeared_out:meanM}: Effects of different system
            sizes at \(\sigma = 0\). Dotted lines are guides to the eye.
        }
        \label{fig:smeared_out}
    \end{figure}\\
    But in the shown plots there occurs no steep decline to zero, but a
    smooth decline. \(\avg{\abs{m}}(T_c) = 0\) is only present in infinite
    systems, hence no computer simulation will show the exact behavior in the
    thermodynamic limit. It will always show some \emph{finte size effects}.
    These finite size effects cause a "smearing out" of the phase
    transition. This is stronger for smaller system sizes, as is visible
    in fig.\ \ref{fig:smeared_out}\subref{sfig:smeared_out:meanM}\footnote{See the appendix \ref{appendix:finiteSizeEffects} for a similar figure for the specific heat.}.
    Clearly the \(L=16\) curve is much less steep than the \(L=128\) curve.\\
    Despite of this one can obtain \(T_c\) by finite size scaling
    methods \cite[p. 232ff]{NewmanBarkema1999}, which also yield the critical
    exponents. This will be performed in the next section.
    Though there is an easier approach, which yields comparable precise
    values for \(T_{c}\) in a much faster and more robust way, which is
    presented in section \ref{ssec:binderIntersections}.

\subsection{Critical Exponents}
    The critical exponents define the course of an observable near its
    divergence. For instance the vicinity of the divergence of the susceptibility
    can be approximated by
    \begin{equation}
        \chi \propto \abs{T-T_{c}}^{-\gamma}
        \label{eq:critExp:gamma}
    \end{equation}
    And the infinite slope of the magnetization can be approximated in
    the direction of lower \(T\) by
    \begin{equation}
        m \propto \abs{T-T_{c}}^{-\beta}
        \label{eq:critExp:beta}
    \end{equation}
    As the name critical \emph{exponent} suggests, this will often
    be a power law, as in both examples above. But not obligatory e.g.\
    the critical "exponent" \(\alpha\) corresponding to the divergence of
    the specific heat \(c\) diverges logarithmic for the two dimensional
    Ising model and is nominally \(\alpha = 0\).
    For that reason, \(\alpha\) will not be considered in this section.\\
    The important property of the critical exponents is, that they are
    universal for a model i.e.\ they are independent of the size, the structure
    or the coupling constant \(J\) \cite{NewmanBarkema1999} of the lattice,
    therefore they should also be independent of \(\sigma\).
    Studies on random lattices with\cite{Lima2000} and without\cite{Janke1994}
    varying coupling constants \(J\) confirm, that the
    critical exponents are not influenced by an random lattice.
    So they will be used for consistency cross checking and
    comparison with the known exact values \cite[p. 59]{Pelissetto2002}.
    If the critical exponents can be reproduced, the determined critical
    temperatures are probably correct too.\\
    % Ausfuehrlichere Herleitung der Skalentheorie, Skalenanalyse
    To determine the critical exponents, the finite size scaling method
    will be used. The here given explanation will give a rough idea, how
    the method works, for a more in depth explanation look at \cite{Norrenbrock2011} or \cite{NewmanBarkema1999}.
    The crucial element is that there exists a \emph{scaling function},
    which is valid for sufficiently large \(L\) near the critical
    temperature \(T_{c}\) and has only an explicit \(L\) dependence.
    One can then express some observables like in eq. \eqref{eq:fsscaling:m}, \eqref{eq:fsscaling:chi} and
    \eqref{eq:fsscaling:g}.
    \begin{align}
        \label{eq:fsscaling:m}
        \avg{m_L} &= L^{-\frac{\beta}{\nu}} \tilde{M}\brac{L^\frac{1}{\nu}\brac{T-T_c}}\\
        \label{eq:fsscaling:chi}
        \chi_L    &= L^{\frac{\gamma}{\nu}} \tilde{C}\brac{L^\frac{1}{\nu}\brac{T-T_c}}\\
        \label{eq:fsscaling:g}
        g         &\propto \tilde{G}\brac{L^\frac{1}{\nu}\brac{T-T_c}}
    \end{align}
    Where \(g\) from eq. \eqref{eq:fsscaling:g} is a normalized
    Binder cumulant (see eq. \eqref{eq:binder}) and \(\tilde{M}, \tilde{C}\) and \(\tilde{G}\)
    are the scaling functions.
    To find the exponent e.g. \(\nu\), one takes \eqref{eq:fsscaling:g},
    solves for \(\tilde{G}\), adjusts the axis to represent \(y=\tilde{G}(x)\),
    and plots the measured observables for all \(L\).
    Then one varies \(\nu\) and \(T_{c}\) until the plotted observables
    collapse on one curve -- the scaling function.
    Like the obersables from fig.\ \ref{fig:gettingCrit}\subref{sfig:gettingCrit:binder_s_0}
    collapse in fig.\ \ref{fig:gettingCrit}\subref{sfig:gettingCrit:collapse_s_0}.
    %\footnote{Examples for a collapse to determine the other critical exponents are in appendix \ref{appendix:finiteSizeScaling}}.
    The same principle can be used to determine the other two exponents.
    Note that \(L=16\) is not used for the collapse, because it is a
    rather small value. Eq. \eqref{eq:fsscaling:m}-\eqref{eq:fsscaling:g}
    are approximations for big \(L\). For small \(L\) one needs some
    \emph{corrections to scaling} terms, which are not considered here.\\
    To accomplish the collapse in an semi-automatic and reproduceable
    way with an error estimate, the program
    \texttt{autoscale.py} \cite{autoscale2009} is used.
    \begin{figure}[htbp]
        \centering
        \subfigure[Binder Cumulant $g$][]
        {
            \label{sfig:gettingCrit:binder_s_0}
            \includegraphics[width=0.47\textwidth]{plots/binder_s_0}
        }
        \subfigure[Finite Size Scaling of the Binder Cumulant $g$][]
        {
            \label{sfig:gettingCrit:collapse_s_0}
            \includegraphics[width=0.47\textwidth]{plots/collapse_s_0}
        }
        \caption[Examples of Determining Critical Temperature and Exponents]
        {
            \subref{sfig:gettingCrit:binder_s_0} The Binder cumulant \(g\)
                of an square lattice Ising model (\(\sigma=0\))\\
            \subref{sfig:gettingCrit:collapse_s_0} collapsed by finite
                size scaling (errors are for clarity not given, see tab.\ \ref{tab:critExp})
        }
        \label{fig:gettingCrit}
    \end{figure}\\
    A finite size scaling analysis was
    performed to determine the critical exponents \(\beta, \gamma, \nu\)
    using \texttt{autoscale.py} \cite{autoscale2009} and eq. \eqref{eq:fsscaling:m}-\eqref{eq:fsscaling:g}.
    The values for \(\nu\) are yielded by every collapse and are therefore
    averaged over all determined values. Fig. \ref{fig:gettingCrit2}\subref{sfig:gettingCrit:s_1_sus}\subref{sfig:gettingCrit:collapse_s_1_sus}
    show an example collapse for \(\sigma=1\) of the magnetic susceptibility
    \begin{equation}
        \chi = \frac{1}{TN}\avg{\avg{m^2} - \avg{m}^2}
    \end{equation}
    This way the exponents \(\gamma\) and \(\nu\) are determined.
    Analogical fig.\ \ref{fig:gettingCrit2}\subref{sfig:gettingCrit:s_1_meanM}\subref{sfig:gettingCrit:collapse_s_1_meanM}
    shows the collapse for the mean absolute magnetization per site \(\avg{\abs{m}}\) for \(\sigma=1\),
    to determine the exponents \(\beta\) and \(\nu\).
    Also the collapse of the binder cumulant as mentioned before and shown
    in fig.\ \ref{fig:gettingCrit}\subref{sfig:gettingCrit:collapse_s_0}
    is used to get a further value of \(\nu\).
    \begin{figure}[htbp]
        \centering
        \subfigure[Susceptibility $\chi$][]
        {
            \label{sfig:gettingCrit:s_1_sus}
            \includegraphics[width=0.47\textwidth]{plots/s_1_sus}
        }
        \subfigure[Finite Size Scaling of the Susceptibility $\chi$][]
        {
            \label{sfig:gettingCrit:collapse_s_1_sus}
            \includegraphics[width=0.47\textwidth]{plots/collapse_s_1_sus}
        }
        \subfigure[Magnetization $\avg{\abs{m}}$][]
        {
            \label{sfig:gettingCrit:s_1_meanM}
            \includegraphics[width=0.47\textwidth]{plots/s_1_meanM}
        }
        \subfigure[Finite Size Scaling of the Magnetization $\avg{\abs{m}}$][]
        {
            \label{sfig:gettingCrit:collapse_s_1_meanM}
            \includegraphics[width=0.47\textwidth]{plots/collapse_s_1_meanM}
        }
        \caption[Examples of Determining Critical Temperature and Exponents]
        {
            \subref{sfig:gettingCrit:s_1_sus} The susceptibility \(\chi\)
                of an Ising model on an RNG \(\sigma=1\)\\
            \subref{sfig:gettingCrit:collapse_s_1_sus} collapsed by finite
                size scaling (for error estimates see tab.\ \ref{tab:critExp})\\
            \subref{sfig:gettingCrit:s_1_meanM} The mean absolute magnetization \(\avg{\abs{m}}\)
                of an Ising model on an GG \(\sigma=1\)\\
            \subref{sfig:gettingCrit:collapse_s_1_meanM} collapsed by finite
                size scaling (for error estimates see tab.\ \ref{tab:critExp})
        }
        \label{fig:gettingCrit2}
    \end{figure}\\
    The values for
    \(\sigma = 0\) are analytically known \cite{Pelissetto2002}. The
    values for all other \(\sigma\) are expected be the same as for
    \(\sigma = 0\) like mentioned before in section \ref{ssec:finitesize}.
    Therefore it is sufficient to take a few samples to test if the
    expectations are met. Hence 5 values of \(\sigma\) are analyzed.
    The analytically known values for \(\sigma = 0\) and the in \cite{Janke1994}
    examined limit of an random lattice \(\sigma \gtrsim 1\) are natural
    choices.
    The other \(\sigma\) are chosen to represent regions, where the behavior of
    \(T_c\) shows some characteristics. (Namely there is a plateau at small
    \(\sigma\), a steep decline at medium \(\sigma\) and a shallow
    decline at \(\sigma = 0.5\). This will be shown in the next
    chapter in fig.\ \ref{fig:Tc}\subref{sfig:Tc:RNG}\subref{sfig:Tc:GG}.)
    The determined values are displayed in tab.\ \ref{tab:critExp}.
    The given errors for \(\beta, \gamma\) are estimates from \texttt{autoscale.py}
    and the error of \(T_c\) and \(\nu\) is the standard deviation of the
    three obtained values.\\
    \begin{table}[htbp]
        \center
        \begin{tabular}{l l l l l l}
            \toprule
             & \multicolumn{1}{c}{\(\sigma\)} & \multicolumn{1}{c}{\(T_c\)} & \multicolumn{1}{c}{\(\nu\)} & \multicolumn{1}{c}{\(\gamma\)} & \multicolumn{1}{c}{\(\beta\)}\\
            \midrule
            exact (\cite[p. 59]{Pelissetto2002}) & \multicolumn{1}{c}{\(0\)} & 2.2691... & \multicolumn{1}{c}{\(1\)} & \multicolumn{1}{c}{\(\frac{7}{4}\)} & \multicolumn{1}{c}{\(\frac{1}{8}\)}\\
            \midrule
            RNG          & 0.0 & 2.2689(7)& 0.992(11)& 1.740(2) & 0.130(1) \\
                         & 0.1 & 2.2058(8)& 0.987(12)& 1.746(5) & 0.133(4) \\
                         & 0.2 & 1.627(2) & 1.010(9) & 1.756(14)& 0.123(10)\\
                         & 0.5 & 1.2825(7)& 1.010(16)& 1.750(16)& 0.143(13)\\
                         & 1.0 & 1.2123(3)& 1.013(6) & 1.758(16)& 0.138(13)\\
            \midrule
            GG           & 0.0 & 2.2687(5)& 0.998(8) & 1.735(2) & 0.1262(4)\\
                         & 0.1 & 2.895(4) & 0.999(19)& 1.744(5) & 0.133(6) \\
                         & 0.3 & 2.527(1) & 1.029(30)& 1.724(16)& 0.129(12)\\
                         & 0.5 & 2.238(1) & 1.006(5) & 1.750(12)& 0.125(13)\\
                         & 1.0 & 2.128(2) & 1.038(32)& 1.743(17)& 0.123(16)\\

            \bottomrule
        \end{tabular}
        \caption[Critical Exponents for Different $\sigma$]{Critical exponents for different $\sigma$}
        \label{tab:critExp}
    \end{table}\\
    Like in tab.\ \ref{tab:critExp} to see, most values
    are matching the expectations. \(T_c\) for \(\sigma = 0\) is in good
    agreement with the known value. Especially \(\nu\) and \(\gamma\)
    are always in very good agreement with their exact values. Besides the
    good agreement of the by data collapse obtained values of \(\nu\) and \(\gamma\),
    the ratio \(\frac{\gamma}{\nu}\) is also determined, by fitting the maxima of
    the susceptibility \(\chi_{\mathrm{max}}\) by the power law \(aL^\frac{\gamma}{\nu}\).
    %~ Note that the maximum of the measured \(\chi\).
    Because there were
    many measurements in the vicinity of \(T_c\) (compare fig.\ \ref{sfig:gettingCrit:s_1_sus}),
    this should give a reasonable estimate, without the need to
    interpolate a maximum.
    The results are displayed in fig.\ \ref{fig:susCrossCheck}. The ratios
    determined by this method confirm the values obtained by collapse.
    \begin{figure}[htbp]
        \centering
        \subfigure[for a RNG][]
        {
            \label{sfig:susCrossCheck:RNG}
            \includegraphics[width=0.45\textwidth]{plots/RNGgammaBySus}
        }
        \subfigure[for a GG][]
        {
            \label{sfig:susCrossCheck:GG}
            \includegraphics[width=0.45\textwidth]{plots/GGgammaBySus}
        }
        \caption[Alternative Way Determining $\gamma$]
        {
            Cross checking the critical exponent $\gamma$ for
                \subref{sfig:susCrossCheck:RNG} the RNG at $\sigma \in \{0.0, 1.0\}$ and
                \subref{sfig:susCrossCheck:GG} the GG at $\sigma \in \{0.0, 0.3\}$.
            Dotted lines are power law fits \(aL^\frac{\gamma}{\nu}\).
        }
        \label{fig:susCrossCheck}
    \end{figure}\\
    Despite the very good results for \(\nu\) and \(\gamma\), most of the
    \(\beta\) seem to be a bit too big -- especially for the RNG -- but
    they are close enough to the expectations to be consistent. At least
    the expectations are always within two times the uncertainty.
    Maybe their deviations can be explained by the fact that small
    systems (\(L=32,64\)) were used for the analysis, and no corrections
    to scaling terms were considered.
    Anyway, two critical exponents are sufficient to determine the
    universality class \cite[p. 145]{Katzgraber2011}. Therefore this
    disordered Ising model is for every \(\sigma\) in the same universality
    class as the standard Ising ferromagnet.

\subsection{Critical Temperature}
\label{ssec:binderIntersections}
    Though if one is just interested in the critical Temperature, an
    easier approach is to find the intersections of the Binder cumulants
    \(g\) of different system sizes \(L\) which are intersecting at
    \(T_c\) \cite{Binder1981}.
    Because the magnetization \(m\) is only measured for discrete values
    of \(T\), \(g\) is also only known for these discrete values and the
    analytical course of the curve is not known, and hence has
    to be interpolated to find the intersection. Therefore a \emph{cubic spline}
    interpolation\footnote{created using the \texttt{scipy.interpolate} tools \cite{scipy2001}}
    is calculated for the measured points.
    Cubic spline interpolation is a piece wise fitting of polynoms of
    degree three which are joined under the condition to be at least two
    times continuously differentiable. This interpolation type has the
    advantage, that it is only influenced by local points so that the
    plateaus at low and high \(T\) do not influence the interpolation in
    the vicinity of \(T_c\) -- in contrast to e.g.\ an polynom fit of
    degree 4, which has to be restricted to the vincinity of \(T_c\) to
    yield meaningful results.
    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.47\textwidth]{plots/binder_fit_s_0}
        \caption[Example of a Binder Cumulant to Determine the Critical Temperature]
        {
            The Binder cumulant \(g\) of an square lattice Ising model
            (\(\sigma=0\)) interpolated with cubic splines
            (the errorbars are too small to see)\\
        }
        \label{fig:gettingCrit:binder_fit_s_0}
    \end{figure}\\
    As an example take fig.\ \ref{fig:gettingCrit:binder_fit_s_0}.
    Here such interpolations are plotted for \(\sigma=0\) and are
    intersecting at \(T \approx 2.27\).
    To determine \(T_c\), the intersections\footnote{found using the \texttt{scipy.optimize} tools \cite{scipy2001}}
    are averaged and the standard error is calculated. In this case one
    gets \(T_c = 2.2689(2)\), which is in good agreement with the
    exact solution from eq. \eqref{eq:exactTc}.
    This test suggests, that measuring \(T_c\) this way is yields
    adequat results and the interpolation does not lead to major
    deviations.\\
    In this section these \(T_c\) on either the RNG or the GG are
    compared. In the following figures, the RNG will always be on the
    left side and the GG on the right side.
    And in tab.\ \ref{tab:critTemp} the values are displayed together with the
    values obtained by the data collapse from the section before.
    \begin{table}[htbp]
        \center
        \begin{tabular}{l l l l l}
            \toprule
             & \multicolumn{2}{c}{RNG} & \multicolumn{2}{c}{GG}\\
            \cmidrule(rl){2-3} \cmidrule(rl){4-5}
            \multicolumn{1}{c}{\(\sigma\)} & \multicolumn{1}{c}{\(T_c\)} & \multicolumn{1}{c}{\(T_c^{\mathrm{collapse}}\)} & \multicolumn{1}{c}{\(T_c\)} & \multicolumn{1}{c}{\(T_c^{\mathrm{collapse}}\)}\\
            \midrule
            0.00 & 2.2690(2)& 2.2689(7)& 2.2689(2)& 2.2687(5\\
            0.03 & 2.2679(4)&          & 2.851(1) &         \\
            0.05 & 2.2662(5)&          & 2.863(1) &         \\
            0.08 & 2.2548(6)&          & 2.882(2) &         \\
            0.10 & 2.205(1) & 2.2058(8)& 2.893(2) & 2.895(4)\\
            0.12 & 2.1010(5)&          & 2.903(3) &         \\
            0.15 & 1.898(2) &          & 2.903(3) &         \\
            0.20 & 1.624(1) & 1.627(2) & 2.8274(6)&         \\
            0.25 & 1.4812(5)&          & 2.676(2) &         \\
            0.30 & 1.407(2) &          & 2.526(4) & 2.527(1)\\
            0.40 & 1.327(4) &          & 2.332(4) &         \\
            0.50 & 1.2818(2)& 1.2825(7)& 2.233(7) & 2.238(1)\\
            0.60 & 1.252(2) &          & 2.183(3) &         \\
            0.70 & 1.234(1) &          & 2.154(8) &         \\
            0.80 & 1.223(1) &          & 2.140(3) &         \\
            0.90 & 1.214(4) &          & 2.132(1) &         \\
            1.00 & 1.208(5) & 1.2123(3)& 2.121(8) & 2.128(2)\\
            1.10 & 1.206(1) &          & 2.116(11)&         \\
            1.20 & 1.204(2) &          & 2.113(4) &         \\
            \bottomrule
        \end{tabular}
        \caption[Critical Temperatures for Different $\sigma$]{Critical temperatures for different $\sigma$}
        \label{tab:critTemp}
    \end{table}\\
    The values obtained through the data collapse and the values obtained
    through the intersection of the binder cumulant are always in good
    agreement and of comparable precision.\\
    These values of \(T_c\) are plotted over \(\sigma\) in fig.\ \ref{fig:Tc}.
    \begin{figure}[htbp]
        \centering
        \subfigure[][]
        {
            \label{sfig:Tc:RNG}
            \includegraphics[width=0.45\textwidth]{plots/RNG_Tc}
        }
        \subfigure[][]
        {
            \label{sfig:Tc:GG}
            \includegraphics[width=0.45\textwidth]{plots/GG_Tc}
        }
        \caption[Critical Temperature over Different Disorder Parameters]
        {
            Critical temperatures \(T_c\) over different
            disorder parameters \(\sigma\) for
            \subref{sfig:Tc:RNG} the RNG and
            \subref{sfig:Tc:GG} the GG.\\
        }
        \label{fig:Tc}
    \end{figure}\\
    One sees that the RNG on the left has generally a lower critical
    temperature than the GG. Also \(T_c\) of the RNG decreases
    monotonically while \(T_c\) of the GG rises at first. Naively one would
    expect, that while changing the displacement of the nodes monotonically,
    the properties of the system will also change monotonically, which is
    indeed the case on a RNG but not on a GG. The
    maximum at \(\sigma \approx 0.15\) on the GG will be discussed later.
    For big \(\sigma\) the displaced nodes approach the limit of randomly
    distributed nodes, hence \(T_c\) is independent of \(\sigma\) for
    \(\sigma \gg 1\). Regarding this both meet the expectations.\\
    Another strange property is the jump from \(\sigma = 0\) to \(\sigma > 0\)
    of \(T_c\) on the GG. To understand that, one has to
    consider the influence of the graph properties on the critical
    temperature.\\
    One basic property of a graph is its degree \(K\) -- sometimes
    called \emph{coordination number}. \(K\) is defined as the mean count
    of neighbors per node.
    \begin{equation}
        K = \frac{1}{N} \sum_{\avg{i,j}} 1
        \label{eq:degree}
    \end{equation}
    It is known, that the degree has an impact on the critical Temperature.
    For example the Honeycomb lattice is of degree \(K=3\)
    and there exists an analytic solution \cite{Wannier1945}.
    Its critical temperature is determined by eq. \eqref{eq:exactHCTc}.
    \begin{equation}
        \cosh\brac{\frac{J}{T_c}}=2 \overset{J=1}{\Longrightarrow} T_c \approx 1.52
        \label{eq:exactHCTc}
    \end{equation}
    This is lower than for the square lattice with degree 4.\\
    If one plots the degree of the graphs at different \(\sigma\) like
    in fig.\ \ref{fig:Tc_deg}\subref{sfig:deg:RNG}\subref{sfig:deg:GG},
    one recognizes, that \(T_c\) and \(K\) are evidently correlated.
    The values for \(K\) are obtained as an average over \(100\)
    realizations of each graph type for \(L=16\) and \(L=32\) lattices.
    Note that \(K\) is independent of \(L\) because of the periodic boundary.
    In fig.\ \ref{fig:Tc_deg}\subref{sfig:deg:RNG}\subref{sfig:deg:GG}
    one can see, that these curves are almost identical and the small
    errorbars suggest, that they are sufficiently precise for this purpose.
    It seems reasonable to normalize \(T_c\) by the degree of the underlying
    graph. This is done in fig.\ \ref{fig:Tc_deg}\subref{sfig:Tc_norm_deg:RNG}\subref{sfig:Tc_norm_deg:GG}.
    Indeed the normalization eliminates the jump and reduces the
    slope of the rising on the GG. Hence it reduces differences
    between the RNG and the GG. But the elimination of the jump is
    unfortunately random and probably caused by a lucky choice of the
    function, which determines the coupling constants \(J\). As proof for
    this claim in appendix \ref{appendix:fixedCoupling} the same analysis
    is performed for a model with fixed \(J\). There the jump gets narrower
    but is still existent. Anyway, the degree seems to have an impact on
    the critical temperature \(T_c\) but it is not a trivial one.
    \begin{figure}[htbp]
        \centering
        \subfigure[][]
        {
            \label{sfig:deg:RNG}
            \includegraphics[width=0.45\textwidth]{plots/RNG_deg}
        }
        \subfigure[][]
        {
            \label{sfig:deg:GG}
            \includegraphics[width=0.45\textwidth]{plots/GG_deg}
        }

        \subfigure[][]
        {
            \label{sfig:Tc_norm_deg:RNG}
            \includegraphics[width=0.45\textwidth]{plots/RNG_Tc_norm_deg}
        }
        \subfigure[][]
        {
            \label{sfig:Tc_norm_deg:GG}
            \includegraphics[width=0.45\textwidth]{plots/GG_Tc_norm_deg}
        }

        \caption[Critical Temperature Normalized by Degree of the Graph]
        {
            Top: Degree \(K\) of graph over different
            disorder parameters \(\sigma\) for
            \subref{sfig:deg:RNG} the RNG and
            \subref{sfig:deg:GG} the GG.\\
            Bottom: Critical temperatures normalized by degree over different
            disorder parameters for
            \subref{sfig:Tc_norm_deg:RNG} the RNG and
            \subref{sfig:Tc_norm_deg:GG} the GG.
        }
        \label{fig:Tc_deg}
    \end{figure}\\
    To understand the jump of \(T_c\), it seems to be necessary to
    understand the jump of \(K\), which is easily explained
    by the definition of the GG and its aftereffects for
    the transition from \(\sigma = 0\). Visible in fig.\ \ref{fig:GG_sigma}\subref{sfig:GG_sigma:0.00}\subref{sfig:GG_sigma:0.01}
    a small change of \(\sigma\) causes many new edges to arise\footnote{See also \url{http://www.youtube.com/watch?v=PcVZ2pG11GI} for an animation.}.
    To fully understand this, take four nodes forming a square. The edge
    across does not exist, because the other two nodes are on the edge
    of the lune. Moving one node slightly into the square, causes the lune
    to get smaller, hence no other nodes are inside or on the edge of
    the lune anymore and the edge appears.\\
    Further the increase of \(T_c\) on the GG can be made plausible.
    Though it is not removed by the normalization, it is reduced. Also
    the maximum of the \(K\) is at the same \(\sigma\), as the maximum
    of \(T_c\). So while displacing the nodes, there arise more edges
    than edges are vanishing until \(\sigma \approx 0.15\) is reached.
    Then more edges disappear, than appear at further displacement. This
    is not an obvious effect, but can be seen in fig.\ \ref{fig:GG_sigma}\subref{sfig:GG_sigma:0.09}\subref{sfig:GG_sigma:0.15}\subref{sfig:GG_sigma:0.21}
    \begin{figure}[htbp]
        \subfigure[][$\sigma = 0.00$]
        {
            \label{sfig:GG_sigma:0.00}
            \includegraphics[width=0.30\textwidth]{images/GG/sigma_e0}
        }
        \subfigure[][$\sigma = 0.01$]
        {
            \label{sfig:GG_sigma:0.01}
            \includegraphics[width=0.30\textwidth]{images/GG/sigma_g0}
        }

        \subfigure[][$\sigma = 0.09$]
        {
            \label{sfig:GG_sigma:0.09}
            \includegraphics[width=0.30\textwidth]{images/GG/out009}
        }
        \subfigure[][$\sigma = 0.15$]
        {
            \label{sfig:GG_sigma:0.15}
            \includegraphics[width=0.30\textwidth]{images/GG/out015}
        }
        \subfigure[][$\sigma = 0.21$]
        {
            \label{sfig:GG_sigma:0.21}
            \includegraphics[width=0.30\textwidth]{images/GG/out021}
        }

        \caption[Examples of GG for different $\sigma$]
        {
            GG with periodic boundary conditions for different \(\sigma = 0\)
        }
        \label{fig:GG_sigma}
    \end{figure}\\
    The evolution of the RNG with increasing \(\sigma\) can be made plausible
    with the same arguments. Fig. \ref{fig:RNG_sigma}
    shows, that for \(\sigma \lesssim 0.1\) the square lattice character is
    preserved -- no new edges arise neither many existing edges vanish, which
    explains the plateau in the \(T_{c}\) diagram. With
    increasing \(\sigma\), more and more edges vanish\footnote{See also \url{http://www.youtube.com/watch?v=rltzi15mTM4} for an animation.},
    thus reducing the degree and \(T_{c}\).
    \begin{figure}[htbp]
        \centering
        \subfigure[][$\sigma = 0.09$]
        {
            \label{sfig:RNG_sigma:0.09}
            \includegraphics[width=0.30\textwidth]{images/RNG/out009}
        }
        \subfigure[][$\sigma = 0.15$]
        {
            \label{sfig:RNG_sigma:0.15}
            \includegraphics[width=0.30\textwidth]{images/RNG/out015}
        }
        \subfigure[][$\sigma = 0.21$]
        {
            \label{sfig:RNG_sigma:0.21}
            \includegraphics[width=0.30\textwidth]{images/RNG/out021}
        }

        \caption[Examples of RNG for different $\sigma$]
        {
            RNG with periodic boundary conditions for different \(\sigma = 0\)
        }
        \label{fig:RNG_sigma}
    \end{figure}\\
    But the degree does not alone influence the behavior of \(T_c\).
    Further \(T_c\) depends on the coupling constant \(J\), which is
    obvious in eq. \eqref{eq:exactTc} and \eqref{eq:exactHCTc}. The
    coupling constant in turn is depending on the length of the edges,
    which changes with \(\sigma\).
    Therefore  in fig.\ \ref{fig:TcJ}\subref{sfig:sumJ:RNG}\subref{sfig:sumJ:GG}
    the mean sum of the coupling constants to all neighbors \(\avg{\sum_{\avg{i,j}} J_{ij}}\)
    is plotted. This is a number which should combine the dependence on
    the degree and the coupling constant. It is determined by summing
    over the edge weights of all edges connected to a node and averaging
    this value over all nodes. Alternatively it is the average edge weight
    of all edges multiplied by the degree.
    The plots fig.\ \ref{fig:TcJ}\subref{sfig:Tc_normJ:RNG}\subref{sfig:Tc_normJ:GG}
    show that \(\avg{\sum_{\avg{i,j}} J_{ij}}\) is also correlated with \(T_c\).
    \begin{figure}[htbp]
        \centering
        \subfigure[][]
        {
            \label{sfig:sumJ:RNG}
            \includegraphics[width=0.45\textwidth]{plots/RNG_sumJ}
        }
        \subfigure[][]
        {
            \label{sfig:sumJ:GG}
            \includegraphics[width=0.45\textwidth]{plots/GG_sumJ}
        }

        \subfigure[][]
        {
            \label{sfig:Tc_normJ:RNG}
            \includegraphics[width=0.45\textwidth]{plots/RNG_Tc_normJ}
        }
        \subfigure[][]
        {
            \label{sfig:Tc_normJ:GG}
            \includegraphics[width=0.45\textwidth]{plots/GG_Tc_normJ}
        }

        \caption[Critical Temperature Normalized by Mean Sum of the Coupling Constants]
        {
            Top: Mean sum of the coupling constants to all
            neighbors over different disorder parameters for
            \subref{sfig:sumJ:RNG} the RNG and
            \subref{sfig:sumJ:GG} the GG.\\
            Bottom: Critical temperatures normalized by mean sum of the
            coupling constants \(\avg{\sum_{\avg{i,j}} J_{ij}}\) over different
            disorder parameters for
            \subref{sfig:Tc_normJ:RNG} the RNG and
            \subref{sfig:Tc_normJ:GG} the GG.
        }
        \label{fig:TcJ}
    \end{figure}\\
    Though eq. \eqref{eq:exactHCTc} shows, that there does not have
    to be a linear connection between \(J\) and \(T_c\), the best guess
    is a linear connection, because this model is derived from the
    square lattice, where the connection is linear. Therefore, one
    normalizes \(T_c\) with \(\avg{\sum_{\avg{i,j}} J_{ij}}\) as in
    fig.\ \ref{fig:TcJ}\subref{sfig:Tc_normJ:RNG}\subref{sfig:Tc_normJ:GG}, the
    jump on the GG arises again, but \(T_c\) is now monotonically
    decreasing with increasing disorder parameter \(\sigma\).
    %~ This is a strong hint, that the change of the \(T_c\) with changing
    %~ \(\sigma\) is in some way connected to \(J\) and \(K\) which is an
    %~ expected conclusion.\\
    Moreover the forms of both curves are quite similar, but the
    one for the RNG in fig.\ \ref{fig:TcJ}\subref{sfig:Tc_normJ:RNG}
    is generally lower and spans over a bigger temperature range than
    the curve of the GG in fig.\ \ref{fig:TcJ}\subref{sfig:Tc_normJ:GG}.
    Both graph types have a plateau at \(0 < \sigma < 0.1\). So small
    disorder has little influence on this normalized critical temperature.
    Also both graph types have a steep decline after the plateau before
    they approach an asymptotic limit for \(\sigma \gg 1\).\\

\subsection{Critical Value of the Binder Cumulant}
    The value of the Binder cumulant at the critical point \(g_c\) is
    depends strongly on boundary conditions but only weakly on the lattice
    structure \cite{BinderValue}. For periodic boundary conditions on a
    square lattice it is \(g_c \approx 0.916\) according to \cite{BinderValue}
        \footnote{Note that \cite{BinderValue} uses another definition of
            the Binder cumulant, and has to be normalized by \(\frac{2}{3}\)
            to match its definition in this thesis.}.
    Because the analysis of section \ref{ssec:binderIntersections}
    yields \(g_c\) anyway, it is easy to check the consistency and
    behavior of \(g_c\) in the disordered Ising model.\\
    \begin{figure}[htbp]
        \centering
        \subfigure[for a RNG][]
        {
            \label{sfig:TcG:RNG}
            \includegraphics[width=0.45\textwidth]{plots/RNG_TcG}
        }
        \subfigure[for a GG][]
        {
            \label{sfig:TcG:GG}
            \includegraphics[width=0.45\textwidth]{plots/GG_TcG}
        }
        \caption[Values of the Binder Cumulant at the Critical Point $g_c$]
        {
            Values of the Binder cumulant at the critical point \(g_c\)
            for\\
            \subref{sfig:TcG:RNG} a RNG and\\
            \subref{sfig:TcG:GG} a GG for different \(\sigma\).
            The dotted line is the reference value for square lattices
            with periodic boundary conditions \cite{BinderValue}, which
            corresponds to \(\sigma = 0\).
        }
        \label{fig:TcG}
    \end{figure}\\
    Considering both plots in fig.\ \ref{fig:TcG}, \(g_c\) is for low
    \(\sigma\) obviously always bigger than the known value. Though the
    deviations are only very small. Perhaps this overestimation is caused
    by the cubic spline interpolation used to acquire these \(g_c\) values.
    For bigger \(\sigma\) the uncertainty gets greater, but the values
    do only differ by a few percent, hence even the big disorder and
    definition of nearest neighbors via a proximity graph does not change
    \(g_c\) much. Though it is mentioned in \cite{BinderValue} that the
    lattice structure has a minor effect on \(g_c\), the uncertainty of
    \(g_c\) is too big, to show it. For a more in depth analysis, new
    Monte Carlo simulations at \(T_c\) would be needed. But this is
    beyond the scope of this thesis. Anyway the results are the expected
    behaivior, because no major deviations from the value of \(g_c\) at
    \(\sigma = 0\) occur at larger \(\sigma\).
