\subsection{Technical Details}
    The generation of the Graphs and the Monte Carlo Simulation are implemented
    in C, all needed random numbers are generated by the GSL \cite{GSL}
    implementation of \emph{Mersenne Twister} \cite{Matsumoto1998} and
    the generated data is evaluated via Python scripts.
    Most simulations were carried out on HERO, the \textbf{H}igh-\textbf{E}nd
    Computing \textbf{R}esource \textbf{O}ldenburg.
    The entire source code is available at \url{https://github.com/surt91/IsingFerromagnet}.\\

    For evaluation the Monte Carlo simulation is run until the system
    is equilibrated after \(t_{eq}\) sweeps. Then the simulation continues
    and the magnetization per site \(m=\frac{1}{N}\sum_i s_i\) and energy
    per site \(E=\frac{1}{N} H\) are calculated and saved for every
    \(2\tau\) sweeps, where \(\tau\) denotes the \emph{autocorrelation time}
    (see also Sec.\ \ref{sssec:eqtime}). The used values for different system sizes
    are listed in Tab.\ \ref{tab:tauAndTeq} in numbers of sweeps. Note that
    the \(t_{eq}\) values are generously rounded up to be on the safe side
    and the \(\tau\) values are determined as the maximum \(\tau\) over all
    simulated temperatures and disorder parameters \(\tau = \underset{T,\sigma}{\max} \{\tau_{T,\sigma}\}\)
    and rounded up to the next integer.
    \begin{table}[htbp]
        \center
        \begin{tabular}{l r r r r r}
            \toprule
            \(L\)    & 16 &  32 &  64 & 128 & 256\\
            \midrule
            \(\tau\) &  3 &   5 &   7 &  12 &  16\\
            \(t_{eq}\) & 40 & 100 & 100 & 200 & 600\\
            \bottomrule
        \end{tabular}
        \caption[Autocorrelation Times $\tau$ and Equilibration Times
            $t_{eq}$ for Different System Sizes $L$]{
            Autocorrelation times $\tau$ and equilibration times
            $t_{eq}$ for different system sizes $L$. All values are generously
            rounded up and determined as the maximum over all \(\sigma\) and \(T\).
        }
        \label{tab:tauAndTeq}
    \end{table}\\
    For every observable \(O\) the expected value \(\avg{O}\) is determined
    as the mean of \(N_{\mathrm{measure}}=10000\) measurements for \(L=16,32\)
    or \(N_{\mathrm{measure}}=5000\) for \(L=64,128\). The number of
    calculated sweeps totals to
    \[N_{\mathrm{sweeps}}=t_{eq}+2\tau N_{\mathrm{measure}}.\]
    The expected values \(\avg{O}\) for \(100\) different random proximity
    graphs with the same disorder parameter \(\sigma\) are then averaged to
    \(\overline{\avg{O}}\). Note that the signs \(\overline{\avg{\cdot}}\)
    are omitted in the following for the sake of simplicity so
    it will be just called \(O\).\\
    Not only the seed to generate the new random
    realization of the proximity graph is changed, but also the seed for
    the random numbers used in the Monte Carlo simulation and during the
    generation of the random start configuration of the spins.\\
    The errors \(\Delta \overline{\avg{O}}\) are estimated by bootstrap resampling
    \cite{Bootstrap}.\\
    The bootstrapping method estimates the error \(\Delta \overline{\avg{O}}\)
    by taking \(M\) random samples from the \(M\) measured values of \(O\),
    where the same value can be chosen more than once, calculating the
    estimator \(\avg{O_b}\) of this bootstrap sample and doing this \(k\)
    times. Then the standard deviation of the \(k\) calculated \(\avg{O_b}\)
    is taken as an estimate for \(\Delta \overline{\avg{O}}\).
    In this thesis the number of bootstrap samples is \(k = 200\).\\
    Note that the error of \(\avg{O}\), which is the standard error\footnote{for the arthmetic mean: standard deviation divided by \(\sqrt M\), where \(M\) is the number of samples}
    of different states on one instance of a proximity graph \(\Delta \avg{O}\),
    is dependend on \(\tau\) \cite[p. 151]{Katzgraber2011}.
    However \(\Delta \overline{\avg{O}}\), which is the the error of the
    observable averaged over different instances of the proximity graphs,
    is not dependend on \(\tau\), because different realizations of the
    random proximity graph are surely uncorrelated.
    Because every error mentioned in this thesis is of this type, it is not
    necessary to determine \(\tau\) for each observable. Therefore a good
    error estimate can be achieved by simple bootstrapping. Nevertheless, one has
    to simulate enough uncorrelated states for each realization to keep
    the \(\Delta \overline{\avg{O}}\) small. For every determined value an
    error is calculated and given in the form \texttt{value(error of last digit)}.
    The errors of fit parameters are the asymptotic standard errors as calculated
    by gnuplot. Gnuplot\footnote{\url{http://gnuplot.info/}.} is an open
    source plotting program used for all plots and fits in this thesis.
    Also note that dotted lines in the plots are -- if not noted otherwise --
    cubic spline interpolations purely meant to be guides to the eye.

\subsection{Short Analysis of the Autocorrelation Time  $\tau$}
\label{ssec:results:autocorr}
    % Sollte das hier rein?
    Before the results are presented, a short analysis of the
    autocorrelation time \(\tau\) according to Eq.\ \ref{eq:tau} is given to
    illustrate the benefits of the used Wolff cluster algorithm.\\
    In Fig.\ \ref{fig:autocorr}\subref{fig:autocorr:temperatures}
    the autocorrelation time for the magnetization per site \(m\) at
    \(\sigma=0\) is plotted. Note that these are the unrounded values
    of Tab.\ \ref{tab:tauAndTeq}.
    \begin{figure}[htbp]
        \centering
        \subfigure[in Dependence on the Temperature $T$][]
        {
            \label{fig:autocorr:temperatures}
            \includegraphics[width=0.45\textwidth]{plots/autocorrT}
        }
        \subfigure[in Dependence on the System Size $L$][]
        {
            \label{fig:autocorr:powerLaw}
            \includegraphics[width=0.45\textwidth]{plots/autocorrL}
        }
        \caption[The Autocorrelation Time $\tau$]
        {
            Dependece of the autocorrelation time $\tau$ on
            \subref{fig:autocorr:temperatures} the temperature $T$ for
                $\sigma=0$ and
            \subref{fig:autocorr:powerLaw} the system size $L$. Plotted on
                double logarithmic axis with a power law fit \(\tau \propto L^z\) (dotted line)
                to determine the dynamical exponent \(z'\).
                (Errorbars are the standard error estimated by bootstrap resampling.)
        }
        \label{fig:autocorr}
    \end{figure}
    The plateau at low temperatures is easy to understand considering the
    effects of the Wolff cluster algorithm. At low temperatures it flips nearly every
    spin in every step, thus the correlation drops to zero after one sweep.
    (Note that this is not the only explanation for the small values of \(\tau\).
    The parallel tempering algorithm swaps spin configurations
    with possibly different signs, thus having the same effect.)
    Also note that the maximum of \(\tau\) is not at \(T_c\) but at a higher
    temperature. The cause is probably the effectiveness of the Wolff cluster
    algorithm at \(T_c\), hence \(\tau \approx 1\) at \(T_{c}\).
    But it is also possible that these are again finite size effects.
    Obviously the autocorrelation time \(\tau\) increases with the system
    size. In fact it obeys a power law \(\tau \propto L^z\), which is
    the expected behavior of a dynamical exponent \(z\) as mentioned in
    Sec.\ \ref{sssec:eqtime}.
    In Fig.\ \ref{fig:autocorr}\subref{fig:autocorr:powerLaw} \(\tau\) is plotted
    over \(L\). Note that the exponent \(z'=0.64(2)\) determined by this plot is
    not comparable to the known critical exponent \(z=0.25\) \cite{NewmanBarkema1999}
    expected for the Wolff cluster algorithm at \(T_c\). On the one hand the number of sweeps,
    in which \(\tau\) is measured in this thesis, are more than a standard
    Metropolis sweep to which \(z\) normally corresponds, because each sweep
    alongside the \(N\) Metropolis flips also a cluster of \(\ge 1\) spin is
    flipped. On the other hand the \(\tau\) used for the fit are not the
    \(\tau\) at \(T_{c}\) but the maximum \(\tau\) of all \(T\).
    Nevertheless it is interesting that it obeys a power law
    \(\tau \propto L^{z'}\). This suggests that \(\xi \gtrsim L\)
    is satisfied at the temperature of maximum \(\tau\), which is plausible,
    because the peak in Fig.\ \ref{fig:autocorr}\subref{fig:autocorr:powerLaw}
    is near \(T_{c}\).\\
    Anyhow, Fig.\ \ref{fig:autocorr}\subref{fig:autocorr:powerLaw} alone
    proofs the effectiveness of the Wolff cluster algorithm at criticality.\\
    This is of course only a rough analysis of existing data which was
    generated for another purpose. For a more detailed inspection, one
    would perform the simulation at the critical point, which will be
    determined subsequently.

\subsection{Finite Size Effects}
\label{ssec:finitesize}
    The aim of this thesis is to find the critical temperature \(T_c\)
    of the disordered Ising model in dependence of the disorder parameter
    \(\sigma\). At \(T_c\) the mean magnetization \(\avg{|m|}\) of
    the system will show a steep decline to zero and the susceptibility
    \begin{equation}
        \chi = \frac{N}{T}\brac{\avg{m^2}-\avg{m}^2}
    \end{equation}
    will diverge. In Fig.\ \ref{fig:smeared_out}\subref{sfig:smeared_out:meanM_L}
    it is easy to see that the steep decline of \(\avg{|m|}\)
    occurs at lower temperatures \(T\) for higher
    disorder parameters \(\sigma\).
    \begin{figure}[htbp]
        \centering
        \subfigure[Dependence of the Phase Transition on $\sigma$][]
        {
            \label{sfig:smeared_out:meanM_L}
            \includegraphics[width=0.45\textwidth]{plots/Mean_M_L_128}
        }
        \subfigure[Example for Finite Size Effects][]
        {
            \label{sfig:smeared_out:meanM}
            \includegraphics[width=0.45\textwidth]{plots/meanM}
        }
        \caption[Phase Transition and Finite Size Effects]
        {
            \subref{sfig:smeared_out:meanM_L} Effects of the disorder
            parameter $\sigma$ on the phase transition
            for an underlying RNG with $L=128$. The position of the slope,
            and hence the critical temperature, moves to lower temperatures
            with increasing \(\sigma\) (\(0.0 \le \sigma \le 1.2\))
            \subref{sfig:smeared_out:meanM} Effects of different system
            sizes at \(\sigma = 0\), i.e.\ the square lattice Ising ferromagnet.
            The \(L=16\) curve is much less steep than the \(L=128\) curve.
        }
        \label{fig:smeared_out}
    \end{figure}\\
    As evident from the figure there occurs no steep decline to zero, but a
    smooth one. \(\avg{\abs{m}}(T_c) = 0\) is only present in infinite
    systems, hence no computer simulation will show the exact behavior in the
    thermodynamic limit. It will always show some \emph{finite size effects}.
    These finite size effects cause a "smearing out" of the phase
    transition. This is stronger for smaller system sizes, as shown
    in Fig.\ \ref{fig:smeared_out}\subref{sfig:smeared_out:meanM}\footnote{See the appendix \ref{appendix:finiteSizeEffects} for a similar figure for the specific heat.}.
    Clearly, the \(L=16\) curve is much less steep than the \(L=128\) curve.\\
    Despite of this one can obtain \(T_c\) by finite size scaling (FSS)
    methods \cite[p. 232ff]{NewmanBarkema1999}, which also yield the critical
    exponents. Such a FSS analysis will be performed in the next section.
    Though there is an easier approach, which yields comparable precise
    values for \(T_{c}\) in a much faster and more robust way, which is
    presented in Sec.\ \ref{ssec:binderIntersections}.

\subsection{Critical Exponents}
\label{ssec:critExp}
    The critical exponents define the behavior of an observable near its
    divergence. For instance the vicinity of the divergence of the susceptibility
    can be approximated by
    \begin{equation}
        \chi \propto \abs{T-T_{c}}^{-\gamma}.
        \label{eq:critExp:gamma}
    \end{equation}
    And the infinite slope of the magnetization can be approximated in
    the direction of lower \(T\) by
    \begin{equation}
        m \propto \abs{T-T_{c}}^{-\beta}.
        \label{eq:critExp:beta}
    \end{equation}
    As the name "critical \emph{exponent}" suggests, the curve progression
    of respective observables will often be a power law close to the critical
    point as in both examples above. But this is not imperative.
    E.g.\ the critical "exponent" \(\alpha\) corresponding to the divergence of
    the specific heat \(c\) diverges logarithmically for the two dimensional
    Ising model and is nominally \(\alpha = 0\).
    For that reason, \(\alpha\) will not be considered in this section.\\
    The important property of the critical exponents is that they are
    universal for a model with respect to certain model characteristics.
    I.e.\ for a given dimension they are independent of the precise lattice
    structure and the magnitude of the coupling constant \(J\).
    Therefore they should also be independent of \(\sigma\). But they are
    not universal regarding the dimension of the model.
    Studies on random lattices with (Ref.\ \cite{Lima2000}) and without (Ref.\ \cite{Janke1994})
    varying coupling constants \(J\) confirm that the
    critical exponents are not influenced by a random structure of the lattice.
    So they will be used for consistency cross checking and
    comparison with the known exact values \cite[p. 59]{Pelissetto2002}.
    If the critical exponents can be reproduced, the determined critical
    temperatures are probably correct, too.\\
    To determine the critical exponents, the FSS method
    will be used. The explanation below will give a rough idea how
    the method works. A more detailed explanation can be found in Refs.\ \cite{Norrenbrock2011}, \cite{DMelchert2009} or \cite{Binder2010}.
    The crucial element is that there exists a \emph{scaling function},
    which is valid for sufficiently large \(L\) near the critical
    temperature \(T_{c}\) and has only an explicit \(L\) dependence.
    One can then express some observables like in Eq.\ \eqref{eq:fsscaling:m}, \eqref{eq:fsscaling:chi} and
    \eqref{eq:fsscaling:g}:
    \begin{align}
        \label{eq:fsscaling:m}
        \avg{m_L} &= L^{-\frac{\beta}{\nu}} \tilde{M}\brac{L^\frac{1}{\nu}\brac{T-T_c}},\\
        \label{eq:fsscaling:chi}
        \chi_L    &= L^{\frac{\gamma}{\nu}} \tilde{C}\brac{L^\frac{1}{\nu}\brac{T-T_c}},\\
        \label{eq:fsscaling:g}
        g         &\propto \tilde{G}\brac{L^\frac{1}{\nu}\brac{T-T_c}}.
    \end{align}
    Where \(g\) in Eq.\ \eqref{eq:fsscaling:g} is the normalized
    Binder cumulant (see Eq.\ \eqref{eq:binder}) and \(\tilde{M}, \tilde{C}\) and \(\tilde{G}\)
    are scaling functions.
    To find the exponent, e.g.\ \(\nu\), one takes \eqref{eq:fsscaling:g},
    solves for \(\tilde{G}\), adjusts the axis to represent \(y=\tilde{G}(x)\),
    and plots the measured observables for all \(L\).
    Then one varies \(\nu\) and \(T_{c}\) until the plotted observables
    collapse on one curve -- the scaling function.
    A data collapse of the curves showed in Fig.\ \ref{fig:gettingCrit}\subref{sfig:gettingCrit:binder_s_0}
    is illustrated in Fig.\ \ref{fig:gettingCrit}\subref{sfig:gettingCrit:collapse_s_0}.
    The same principle can be used to determine the other two exponents.
    Note that \(L=16\) is not used for the collapse, because it is a
    rather small value of \(L\) for which deviations from the assumed
    scaling behavior might be expected. I.e.\ Eq.\ \eqref{eq:fsscaling:m}-\eqref{eq:fsscaling:g}
    are approximations for big \(L\). For small \(L\) one needs some
    \emph{corrections to scaling} terms, which are not considered here.\\
    To accomplish the collapse in a semi-automatic and reproduceable
    way with an error estimate, the program
    \texttt{autoscale.py} \cite{autoscale2009} is used.
    \begin{figure}[htbp]
        \centering
        \subfigure[Binder Cumulant $g$][]
        {
            \label{sfig:gettingCrit:binder_s_0}
            \includegraphics[width=0.47\textwidth]{plots/binder_s_0}
        }
        \subfigure[Finite Size Scaling of the Binder Cumulant $g$][]
        {
            \label{sfig:gettingCrit:collapse_s_0}
            \includegraphics[width=0.47\textwidth]{plots/collapse_s_0}
        }
        \caption[Examples of Determining Critical Temperature and Exponents]
        {
            \subref{sfig:gettingCrit:binder_s_0} The Binder cumulant \(g\)
                of an square lattice Ising model (\(\sigma=0\)).
            \subref{sfig:gettingCrit:collapse_s_0} The curve from \subref{sfig:gettingCrit:binder_s_0}
                collapsed by FSS (errors are for clarity
                not given, see Tab.\ \ref{tab:critExp})
        }
        \label{fig:gettingCrit}
    \end{figure}\\
    A FSS analysis was
    performed to determine the critical exponents \(\beta, \gamma, \nu\)
    using \texttt{autoscale.py} \cite{autoscale2009} and Eq.\ \eqref{eq:fsscaling:m}-\eqref{eq:fsscaling:g}.
    The values for \(\nu\) are obtained by data collapse for each observable.
    As a final result, the arithmetic mean of these values is presented.
    Fig. \ref{fig:gettingCrit2}\subref{sfig:gettingCrit:s_1_sus}\subref{sfig:gettingCrit:collapse_s_1_sus}
    show an examplary collapse for \(\sigma=1\) of the magnetic susceptibility
    \begin{equation}
        \chi = \frac{1}{TN}\avg{\avg{m^2} - \avg{m}^2}.
    \end{equation}
    This way the exponents \(\gamma\) and \(\nu\) are determined.
    Similarly Fig.\ \ref{fig:gettingCrit2}\subref{sfig:gettingCrit:s_1_meanM}\subref{sfig:gettingCrit:collapse_s_1_meanM}
    show the collapse for the mean magnetization per site \(\avg{\abs{m}}\) for \(\sigma=1\)
    to determine the exponents \(\beta\) and \(\nu\).
    Also the collapse of the binder cumulant, as mentioned before and shown
    in Fig.\ \ref{fig:gettingCrit}\subref{sfig:gettingCrit:collapse_s_0},
    is used to get a further estimate of \(\nu\).
    \begin{figure}[htbp]
        \centering
        \subfigure[Susceptibility $\chi$][]
        {
            \label{sfig:gettingCrit:s_1_sus}
            \includegraphics[width=0.47\textwidth]{plots/s_1_sus}
        }
        \subfigure[Finite Size Scaling of the Susceptibility $\chi$][]
        {
            \label{sfig:gettingCrit:collapse_s_1_sus}
            \includegraphics[width=0.47\textwidth]{plots/collapse_s_1_sus}
        }
        \subfigure[Magnetization $\avg{\abs{m}}$][]
        {
            \label{sfig:gettingCrit:s_1_meanM}
            \includegraphics[width=0.47\textwidth]{plots/s_1_meanM}
        }
        \subfigure[Finite Size Scaling of the Magnetization $\avg{\abs{m}}$][]
        {
            \label{sfig:gettingCrit:collapse_s_1_meanM}
            \includegraphics[width=0.47\textwidth]{plots/collapse_s_1_meanM}
        }
        \caption[Examples of Determining Critical Temperature and Exponents]
        {
            Examples for the method of FSS, which was used
            to determine the critical exponents \(\nu, \gamma, \beta\) and
            the critical temperature \(T_c\).
            \subref{sfig:gettingCrit:s_1_sus} The susceptibility \(\chi\)
                of an Ising model on an RNG at \(\sigma=1\).
            \subref{sfig:gettingCrit:collapse_s_1_sus} data collapse, to determine \(\gamma\) (for error estimates see Tab.\ \ref{tab:critExp}).
            \subref{sfig:gettingCrit:s_1_meanM} The mean magnetization \(\avg{\abs{m}}\)
                of an Ising model on an GG at \(\sigma=1\).
            \subref{sfig:gettingCrit:collapse_s_1_meanM} data collapse, to determine \(\beta\) (for error estimates see Tab.\ \ref{tab:critExp}).
        }
        \label{fig:gettingCrit2}
    \end{figure}\\
    The values for \(\sigma = 0\), i.e.\ the critical temperature and
    critical exponents for the square lattice Ising model,
    are analytically known \cite{Pelissetto2002}. Due to universality, the
    values for all other \(\sigma\) are expected be the same as for
    \(\sigma = 0\) like mentioned before in Sec.\ \ref{ssec:finitesize}.
    Therefore it is sufficient to take a few samples to test, if the
    expectations match. Hence 5 values of \(\sigma\) are analyzed.
    The analytically known values for \(\sigma = 0\) and the
    limit of a random lattice \(\sigma \gtrsim 1\), examined in
    Ref.\ \cite{Janke1994}, are natural choices.
    The other \(\sigma\) are chosen to represent regions where the behavior of
    \(T_c\) shows some characteristics. (As we will see later, there is a plateau at small
    \(\sigma\), a steep decline at intermediate values of \(\sigma\) and a shallow
    decline at \(\sigma = 0.5\). This will be shown in the next
    chapter in Fig.\ \ref{fig:Tc}\subref{sfig:Tc:RNG}\subref{sfig:Tc:GG}.)
    The range \(\Delta x\) which specifies the mesurements used for the
    data collapse, was restricted to the in Tab.\ \ref{tab:FSS_range}
    listed ranges.
    \begin{table}[htbp]
        \center
        \begin{tabular}{c l l@{, }r}
            \toprule
            collapsed observable & \multicolumn{1}{c}{\(\sigma\) }   & \multicolumn{2}{c}{\(\Delta x\)}\\
            \midrule
            \(\avg{\abs{m}}\)    & \(0.0, 0.1\)                      & \([-2.5\) & \( 7]\)\\
                                 & \(0.2, 0.3, 0.5, 1.0\)           & \([-1.5\) & \(10]\)\\
            \(\chi\)             & \(0.0, 0.1\)                      & \([-2.0\) & \( 7]\)\\
                                 & \(0.2, 0.3, 0.5, 1.0\)           & \([-1.5\) & \( 7]\)\\
            \(g\)                & \(0.0, 0.1\)                      & \([-3.5\) & \(12]\)\\
                                 & \(0.2, 0.3, 0.5, 1.0\)           & \([-1.5\) & \(10]\)\\
            \bottomrule
        \end{tabular}
        \caption[Ranges of the Data Collapse]{
            The range \(\Delta x\) specifies a range on the \(x\)-axis of
            the plots after the collapse. Values inside this range are
            considered to judge the quality of the collapse. I.e.\ only
            the data points inside this range have to collapse on each
            other -- data points beyond do not have to. This accounts
            for the fact, that FSS is only near the critical point a good
            approximation.
        }
        \label{tab:FSS_range}
    \end{table}\\
    The determined values are displayed in Tab.\ \ref{tab:critExp}.
    The given errors for \(\beta, \gamma\) are estimates from \texttt{autoscale.py}
    and the errors of \(T_c\) and \(\nu\) are the standard deviations of
    three obtained values.\\
    \begin{table}[htbp]
        \center
        \begin{tabular}{l l l l l l}
            \toprule
             & \multicolumn{1}{c}{\(\sigma\)} & \multicolumn{1}{c}{\(T_c\)} & \multicolumn{1}{c}{\(\nu\)} & \multicolumn{1}{c}{\(\gamma\)} & \multicolumn{1}{c}{\(\beta\)}\\
            \midrule
            exact (\cite[p. 59]{Pelissetto2002}) & \multicolumn{1}{c}{\(0\)} & 2.2691... & \multicolumn{1}{c}{\(1\)} & \multicolumn{1}{c}{\(\frac{7}{4}\)} & \multicolumn{1}{c}{\(\frac{1}{8}\)}\\
            \midrule
            RNG          & 0.0 & 2.2689(7)& 0.992(11)& 1.740(2) & 0.130(1) \\
                         & 0.1 & 2.2058(8)& 0.987(12)& 1.746(5) & 0.133(4) \\
                         & 0.2 & 1.627(2) & 1.010(9) & 1.756(14)& 0.123(10)\\
                         & 0.5 & 1.2825(7)& 1.010(16)& 1.750(16)& 0.143(13)\\
                         & 1.0 & 1.2123(3)& 1.013(6) & 1.758(16)& 0.138(13)\\
            \midrule
            GG           & 0.0 & 2.2687(5)& 0.998(8) & 1.735(2) & 0.1262(4)\\
                         & 0.1 & 2.895(4) & 0.999(19)& 1.744(5) & 0.133(6) \\
                         & 0.3 & 2.527(1) & 1.029(30)& 1.724(16)& 0.129(12)\\
                         & 0.5 & 2.238(1) & 1.006(5) & 1.750(12)& 0.125(13)\\
                         & 1.0 & 2.128(2) & 1.038(32)& 1.743(17)& 0.123(16)\\

            \bottomrule
        \end{tabular}
        \caption[Critical Exponents for Different $\sigma$]{
            Critical exponents for different values of \(\sigma\). A finite size
            scaling analysis was performed to determine the critical
            exponents \(\beta, \gamma, \nu\) and the critical temperature
            \(T_c\). The errors for \(\beta\) and \(\gamma\) are estimated
            by \texttt{autoscale.py} \cite{autoscale2009}. The errors of
            \(\nu\) and \(T_c\) are the standard deviation of three obtained
            values through different collapses (see text).
        }
        \label{tab:critExp}
    \end{table}\\
    According to Tab.\ \ref{tab:critExp}, most values
    are matching the expectations. \(T_c\) for \(\sigma = 0\) is in good
    agreement with the known value. Especially \(\nu\) and \(\gamma\)
    are always in very good agreement with their exact values. Besides the
    good agreement of the values of \(\nu\) and \(\gamma\) obtained by data collapse,
    the ratio \(\frac{\gamma}{\nu}\) is also determined by fitting the maxima of
    the susceptibility \(\chi_{\mathrm{max}}\) to the power law function \(aL^\frac{\gamma}{\nu}\).
    Because there were many measurements in the vicinity of \(T_c\)
    (c.f.\ Fig.\ \ref{sfig:gettingCrit:s_1_sus}), it should give a
    reasonable estimate to take their maximum, without the need to
    interpolate.
    The results are displayed in Fig.\ \ref{fig:susCrossCheck}. The ratios
    determined by this method confirm the values obtained by collapse.
    \begin{figure}[htbp]
        \centering
        \subfigure[for a RNG][]
        {
            \label{sfig:susCrossCheck:RNG}
            \includegraphics[width=0.45\textwidth]{plots/RNGgammaBySus}
        }
        \subfigure[for a GG][]
        {
            \label{sfig:susCrossCheck:GG}
            \includegraphics[width=0.45\textwidth]{plots/GGgammaBySus}
        }
        \caption[Alternative Way Determining $\gamma / \nu$]
        {
            Cross checking the ratio of the critical exponents $\gamma$ and $\nu$ for
                \subref{sfig:susCrossCheck:RNG} the RNG at $\sigma \in \{0.0, 1.0\}$ and
                \subref{sfig:susCrossCheck:GG} the GG at $\sigma \in \{0.0, 0.3\}$.
            The plotted values are the maxima of all measured \(\chi\).
            Dotted lines are fits to the power law function \(aL^\frac{\gamma}{\nu}\).
        }
        \label{fig:susCrossCheck}
    \end{figure}\\
    Despite the very good results for \(\nu\) and \(\gamma\), most of the
    \(\beta\) seem to be a bit too big -- especially for the RNG -- but
    they are close enough to the expectations to be consistent. At least
    the expectations are always within two times the uncertainty.
    Maybe their deviations can be explained by the fact that small
    systems (\(L=32,64\)) were used for the analysis and no corrections
    to scaling terms were considered.
    Anyway, two critical exponents are sufficient to determine the
    universality class \cite[p. 145]{Katzgraber2011}. Therefore,
    the Ising model on a proximity graph is for every \(\sigma\) in the
    same universality class as the square lattice Ising ferromagnet.

\subsection{Critical Temperature}
\label{ssec:binderIntersections}
    Though, if one is just interested in the critical temperature, an
    easier approach is to find the intersections of the Binder cumulants
    \(g\) of different system sizes \(L\), which intersect at \(T_c\) \cite{Binder1981}.
    Because the magnetization \(m\) is only measured for discrete values
    of \(T\), \(g\) is also only known for these discrete values and the
    analytical course of the curve is not known and hence has
    to be interpolated to find the intersection. Therefore a \emph{cubic spline}
    interpolation\footnote{created using the \texttt{scipy.interpolate} tools \cite{scipy2001}}
    is calculated for the measured points.
    Cubic spline interpolation is a piece wise fitting of polynoms of
    degree three which are joined under the condition to be at least two
    times continuously differentiable. This interpolation type has the
    advantage that it is only influenced by local points so that the
    plateaus at low and high \(T\) do not influence the interpolation in
    the vicinity of \(T_c\) -- in contrast to, e.g.\ an polynom fit of
    degree 4, which has to be restricted to the vincinity of \(T_c\) to
    yield meaningful results.
    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.47\textwidth]{plots/binder_fit_s_0}
        \caption[Example of a Binder Cumulant to Determine the Critical Temperature]
        {
            The Binder cumulant \(g\) of an square lattice Ising model
            (\(\sigma=0\)) interpolated with cubic splines, to determine
            the intersection, which is at \(T_c\)
            (the errorbars are too small to see).
        }
        \label{fig:gettingCrit:binder_fit_s_0}
    \end{figure}\\
    Take Fig.\ \ref{fig:gettingCrit:binder_fit_s_0} as an example.
    Here such interpolations are plotted for \(\sigma=0\) and are
    intersecting at \(T \approx 2.27\).
    To determine \(T_c\), the intersections\footnote{found using the \texttt{scipy.optimize} tools \cite{scipy2001}}
    are averaged and the standard error is calculated. In this case, one
    gets \(T_c = 2.2689(2)\), which is in good agreement with the
    exact solution from Eq.\ \eqref{eq:exactTc}.
    This test suggests that measuring \(T_c\) this way yields
    adequat results and the interpolation does not lead to major
    deviations.\\
    In this section \(T_c\) for both the RNG or the GG are
    compared. In the following figures the RNG will always be on the
    left side and the GG on the right side.
    And in Tab.\ \ref{tab:critTemp} the values are displayed together with the
    values obtained by the data collapse from the previous section.
    \begin{table}[htbp]
        \center
        \begin{tabular}{l l l l l}
            \toprule
             & \multicolumn{2}{c}{RNG} & \multicolumn{2}{c}{GG}\\
            \cmidrule(rl){2-3} \cmidrule(rl){4-5}
            \multicolumn{1}{c}{\(\sigma\)} & \multicolumn{1}{c}{\(T_c\)} & \multicolumn{1}{c}{\(T_c^{\mathrm{collapse}}\)} & \multicolumn{1}{c}{\(T_c\)} & \multicolumn{1}{c}{\(T_c^{\mathrm{collapse}}\)}\\
            \midrule
            0.00 & 2.2690(2)& 2.2689(7)& 2.2689(2)& 2.2687(5)\\
            0.03 & 2.2679(4)&          & 2.851(1) &         \\
            0.05 & 2.2662(5)&          & 2.863(1) &         \\
            0.08 & 2.2548(6)&          & 2.882(2) &         \\
            0.10 & 2.205(1) & 2.2058(8)& 2.893(2) & 2.895(4)\\
            0.12 & 2.1010(5)&          & 2.903(3) &         \\
            0.15 & 1.898(2) &          & 2.903(3) &         \\
            0.20 & 1.624(1) & 1.627(2) & 2.8274(6)&         \\
            0.25 & 1.4812(5)&          & 2.676(2) &         \\
            0.30 & 1.407(2) &          & 2.526(4) & 2.527(1)\\
            0.40 & 1.327(4) &          & 2.332(4) &         \\
            0.50 & 1.2818(2)& 1.2825(7)& 2.233(7) & 2.238(1)\\
            0.60 & 1.252(2) &          & 2.183(3) &         \\
            0.70 & 1.234(1) &          & 2.154(8) &         \\
            0.80 & 1.223(1) &          & 2.140(3) &         \\
            0.90 & 1.214(4) &          & 2.132(1) &         \\
            1.00 & 1.208(5) & 1.2123(3)& 2.121(8) & 2.128(2)\\
            1.10 & 1.206(1) &          & 2.116(11)&         \\
            1.20 & 1.204(2) &          & 2.113(4) &         \\
            \bottomrule
        \end{tabular}
        \caption[Critical Temperatures for Different $\sigma$]{
            Critical temperatures for different $\sigma$. For both graph
            types, GG and RNG. \(T_c^{\mathrm{collapse}}\) denotes the
            values of \(T_c\) which were determined before via finte
            size scaling analysis. The other values are determined by
            the intersection of the binder cumulants \(g\) for different
            system sizes. The values of both methods match.
        }
        \label{tab:critTemp}
    \end{table}\\
    The values obtained through the data collapse and the values obtained
    through the intersection of the binder cumulant are always in good
    agreement and of comparable precision.\\
    These values of \(T_c\) are plotted over \(\sigma\) in Fig.\ \ref{fig:Tc}.
    \begin{figure}[htbp]
        \centering
        \subfigure[][]
        {
            \label{sfig:Tc:RNG}
            \includegraphics[width=0.45\textwidth]{plots/RNG_Tc}
        }
        \subfigure[][]
        {
            \label{sfig:Tc:GG}
            \includegraphics[width=0.45\textwidth]{plots/GG_Tc}
        }
        \caption[Critical Temperature over Different Disorder Parameters]
        {
            Critical temperatures \(T_c\) over different
            disorder parameters \(\sigma\) for
            \subref{sfig:Tc:RNG} the RNG and
            \subref{sfig:Tc:GG} the GG.
            Interesting points are the jump and rise of the GG at small
            \(\sigma\) and the plateau on the RNG for small \(\sigma\).
        }
        \label{fig:Tc}
    \end{figure}\\
    One sees that the RNG on the left has generally a lower critical
    temperature than the GG.
    Also \(T_c\) of the RNG decreases
    monotonically while \(T_c\) of the GG rises at first. Naively one would
    expect that while changing the displacement of the nodes monotonically,
    the properties of the system will also change monotonically, which is
    indeed the case on a RNG but not on a GG. The
    maximum at \(\sigma \approx 0.15\) on the GG will be discussed later.
    For large values of \(\sigma\) the displaced nodes approach the limit of randomly
    distributed nodes, hence \(T_c\) is independent of \(\sigma\) for
    \(\sigma \gg 1\). Regarding this both graph types meet the expectations.\\
    Another strange property is the jump from \(\sigma = 0\) to \(\sigma > 0\)
    of \(T_c\) on the GG. To understand that one has to
    consider the influence of the graph properties on the critical
    temperature.

    \subsubsection{Influence of the Average Degree on the Critical Temperature}
        One basic property of a graph is its average \emph{degree} \(K\) -- sometimes
        called average \emph{coordination number}. \(K\) is defined as the mean count
        of neighbors per node.
        \begin{equation}
            K = \frac{1}{N} \sum_{\avg{i,j}} 1.
            \label{eq:degree}
        \end{equation}
        For a Poisson point process, i.e.\ for \(\sigma \gtrsim 1\),
        the average degree of the mentioned graph types are known.
        \(K_{DT} = 6\) \cite{DelaunayDegree}, \(K_{GG} = 4\) \cite{notPChristoph} and \(K_{RNG} = 2.5576(3)\) \cite{RNGCell}.
        Indeed, for \(\sigma \gtrsim 1\) Fig.\ \ref{fig:Tc_deg}\subref{sfig:deg:GG}\subref{sfig:deg:RNG}
        confirm the last two values.\\
        It is well known that the degree has an impact on the critical temperature.
        For example the Honeycomb lattice is of degree \(K=3\) and the
        respective critical temperature can be obtained by analytic means \cite{Wannier1945},
        yielding
        \begin{equation}
            \cosh\brac{\frac{J}{T_c}}=2 \overset{J=1}{\Longrightarrow} T_c \approx 1.52.
            \label{eq:exactHCTc}
        \end{equation}
        Note that this is lower than the critical temperature \(T_c = 2.269...\)
        characteristic for the square lattice with degree \(K=4\).\\
        This is plausible, because more edges lead to more neighbors.
        Consider, e.g., a fully polarized spin configuration. Now, the
        energy needed to flip a spin so that it assumes an orientation opposite
        to the orientation of its neighbors increases with the number of
        its neighbors. So the more edges are in
        the graph the more stable the system becomes with respect to spin flips
        at low \(T\), leading to an increasing value of \(T_c\).
        This is also an explanation why the \(T_c\) of the RNG is always
        lower than that of the GG: The degree of the RNG is lower.\\
        If one plots the degree of the graphs at different \(\sigma\) like
        in Fig.\ \ref{fig:Tc_deg}\subref{sfig:deg:RNG}\subref{sfig:deg:GG},
        one recognizes that \(T_c\) and \(K\) are evidently correlated.
        The values for \(K\) are obtained as an average over \(100\)
        realizations of each graph type for \(L=16\) and \(L=32\) lattices.
        %~ Note that \(K\) is independent of \(L\) because of the periodic boundary.
        In Fig.\ \ref{fig:Tc_deg}\subref{sfig:deg:RNG}\subref{sfig:deg:GG}
        one can see that these curves are almost identical and the small
        errorbars suggest that they are sufficiently precise for this purpose.
        It seems reasonable to normalize \(T_c\) by the degree of the underlying
        graph. This is done in Fig.\ \ref{fig:Tc_deg}\subref{sfig:Tc_norm_deg:RNG}\subref{sfig:Tc_norm_deg:GG}.
        Indeed the normalization eliminates the jump and reduces the
        slope of the \(T_c\) curve for the GG. Hence it reduces differences
        between the RNG and the GG. However the elimination of the jump is
        unfortunately a coincidence and probably caused by a lucky choice of the
        function, which determines the coupling constants \(J\). As proof for
        this claim in Sec.\ \ref{appendix:fixedCoupling} the same analysis
        is performed for a model with fixed \(J\). There the jump gets narrower
        but is still existent. Anyway, the degree seems to have an impact on
        the critical temperature \(T_c\), but it is not a trivial one.
        Note that even after the normalization, the values of \(T_c / K\)
        for the RNG are smaller than those for the GG.
        \begin{figure}[htbp]
            \centering
            \subfigure[][]
            {
                \label{sfig:deg:RNG}
                \includegraphics[width=0.45\textwidth]{plots/RNG_deg}
            }
            \subfigure[][]
            {
                \label{sfig:deg:GG}
                \includegraphics[width=0.45\textwidth]{plots/GG_deg}
            }

            \subfigure[][]
            {
                \label{sfig:Tc_norm_deg:RNG}
                \includegraphics[width=0.45\textwidth]{plots/RNG_Tc_norm_deg}
            }
            \subfigure[][]
            {
                \label{sfig:Tc_norm_deg:GG}
                \includegraphics[width=0.45\textwidth]{plots/GG_Tc_norm_deg}
            }

            \caption[Critical Temperature Normalized by Degree of the Graph]
            {
                Top: Degree \(K\) of graph over different
                disorder parameters \(\sigma\) for
                \subref{sfig:deg:RNG} the RNG and
                \subref{sfig:deg:GG} the GG.
                Bottom: Critical temperatures normalized by degree over different
                disorder parameters for
                \subref{sfig:Tc_norm_deg:RNG} the RNG and
                \subref{sfig:Tc_norm_deg:GG} the GG.
            }
            \label{fig:Tc_deg}
        \end{figure}\\
        To understand the jump of \(T_c\) it seems to be necessary to
        understand the jump of \(K\), which is easily explained
        by the definition of the GG. As evident from Fig.\ \ref{fig:GG_sigma}\subref{sfig:GG_sigma:0.00}\subref{sfig:GG_sigma:0.01}
        a small change of \(\sigma\) causes many new edges to arise\footnote{See also \url{http://www.youtube.com/watch?v=PcVZ2pG11GI} for an animation.}.
        To fully understand this, take four nodes forming a square. The edge
        across the diagonal of the plaquette does not exist, because the
        other two nodes are located exactly on the border
        of the lune. Moving one node slightly into the square, causes the lune
        to get smaller, hence no other nodes are inside or on the border of
        the lune anymore and the new diagonal edge appears. This process is sketched in
        Fig.\ \ref{fig:GGEdge}.
        \begin{figure}[htbp]
            \centering
            \subfigure[][]{
                \label{sfig:GGEdge:before}
                \input{images/GGEdgeBefore}
            }
            \subfigure[][]{
                \label{sfig:GGEdge:after}
                \input{images/GGEdgeAfter}
            }
            \caption[Sketch why Many New Edges Arise at the Transition from $\sigma = 0$ to $\sigma > 0$]
            {
                \subref{sfig:GGEdge:before} The edge across does not exist,
                because the other two nodes are on the edge of the lune.
                \subref{sfig:GGEdge:after} Moving one node slightly into the
                square, causes the lune to get smaller, hence no other nodes
                are inside or on the edge of the lune anymore and the edge
                appears.
            }
            \label{fig:GGEdge}
        \end{figure}\\
        Further, the increase of \(T_c\) on the GG can be made plausible.
        Also, the maximum value of \(K\) and the minimum value of \(T_c\)
        are observed at a similar value of \(\sigma\). Hence, while displacing the nodes, there arise more edges
        than edges are vanishing until \(\sigma \approx 0.15\) is reached.
        Then more edges disappear, than appear at further displacement. This
        is not an obvious effect, but can be seen in Fig.\ \ref{fig:GG_sigma}\subref{sfig:GG_sigma:0.09}\subref{sfig:GG_sigma:0.15}\subref{sfig:GG_sigma:0.21}.
        \begin{figure}[htbp]
            \subfigure[][$\sigma = 0.00$]
            {
                \label{sfig:GG_sigma:0.00}
                \includegraphics[width=0.30\textwidth]{images/GG/sigma_e0}
            }
            \subfigure[][$\sigma = 0.01$]
            {
                \label{sfig:GG_sigma:0.01}
                \includegraphics[width=0.30\textwidth]{images/GG/sigma_g0}
            }

            \subfigure[][$\sigma = 0.09$]
            {
                \label{sfig:GG_sigma:0.09}
                \includegraphics[width=0.30\textwidth]{images/GG/out009}
            }
            \subfigure[][$\sigma = 0.15$]
            {
                \label{sfig:GG_sigma:0.15}
                \includegraphics[width=0.30\textwidth]{images/GG/out015}
            }
            \subfigure[][$\sigma = 0.21$]
            {
                \label{sfig:GG_sigma:0.21}
                \includegraphics[width=0.30\textwidth]{images/GG/out021}
            }

            \caption[Examples of GG for different $\sigma$]
            {
                GG with periodic boundary conditions for different \(\sigma\).
                The number of edges increases from \subref{sfig:GG_sigma:0.00}
                to \subref{sfig:GG_sigma:0.01} significantly. Until \subref{sfig:GG_sigma:0.15}
                it increases and after that the number of edges decreases.
            }
            \label{fig:GG_sigma}
        \end{figure}\\
        The evolution of the RNG with increasing \(\sigma\) can be made plausible
        with the same arguments. Fig. \ref{fig:RNG_sigma}
        shows that for \(\sigma \lesssim 0.1\) the square lattice character is
        preserved -- no new edges arise and only a few existing edges vanish, which
        explains the plateau in the \(T_{c}\) diagram. With
        increasing \(\sigma\), more and more edges vanish\footnote{See also \url{http://www.youtube.com/watch?v=rltzi15mTM4} for an animation.},
        thus reducing the degree and consequently the corresponding value of \(T_{c}\).
        \begin{figure}[htbp]
            \centering
            \subfigure[][$\sigma = 0.09$]
            {
                \label{sfig:RNG_sigma:0.09}
                \includegraphics[width=0.30\textwidth]{images/RNG/out009}
            }
            \subfigure[][$\sigma = 0.15$]
            {
                \label{sfig:RNG_sigma:0.15}
                \includegraphics[width=0.30\textwidth]{images/RNG/out015}
            }
            \subfigure[][$\sigma = 0.21$]
            {
                \label{sfig:RNG_sigma:0.21}
                \includegraphics[width=0.30\textwidth]{images/RNG/out021}
            }

            \caption[Examples of RNG for different $\sigma$]
            {
                RNG with periodic boundary conditions for different \(\sigma\).
                \subref{sfig:RNG_sigma:0.09} has still almost the square lattice
                configuration of edges, which is why the plateau from Fig.\ \ref{fig:Tc}\subref{sfig:Tc:RNG}
                exists. In the next two pictures one sees the fast disappearance
                of edges, chracteristic for the RNG for a set of randomly
                distributed points (i.e.\ Poisson process).
            }
            \label{fig:RNG_sigma}
        \end{figure}

        \clearpage

    \subsubsection{Influence of the Coupling Constant on the Critical Temperature}
    \label{sssec:J}
        However, note that the degree does not alone influence the behavior of \(T_c\).
        Further \(T_c\) depends on the coupling constant \(J\), which is
        obvious from Eqs.\ \eqref{eq:exactTc} and \eqref{eq:exactHCTc}. The
        coupling constant in turn is depending on the length of the edges,
        which changes with \(\sigma\).
        Therefore  in Fig.\ \ref{fig:TcJ}\subref{sfig:sumJ:RNG}\subref{sfig:sumJ:GG}
        the mean sum of the coupling constants to all neighbors
        \begin{equation}
            \avg{\sum_{\avg{i,j}} J_{ij}}
        \end{equation}
        is plotted. This is a number which should combine the dependence on
        the degree and the coupling constant. It is determined by summing
        over the edge weights of all edges connected to a node and averaging
        this value over all nodes. Alternatively it is the average edge weight
        of all edges of the graph multiplied by the degree \(\avg{J_{ij}} K\).
        The plots Fig.\ \ref{fig:TcJ}\subref{sfig:Tc_normJ:RNG}\subref{sfig:Tc_normJ:GG}
        show that \(\avg{\sum_{\avg{i,j}} J_{ij}}\) is also correlated with \(T_c\).
        \begin{figure}[htbp]
            \centering
            \subfigure[][]
            {
                \label{sfig:sumJ:RNG}
                \includegraphics[width=0.45\textwidth]{plots/RNG_sumJ}
            }
            \subfigure[][]
            {
                \label{sfig:sumJ:GG}
                \includegraphics[width=0.45\textwidth]{plots/GG_sumJ}
            }

            \subfigure[][]
            {
                \label{sfig:Tc_normJ:RNG}
                \includegraphics[width=0.45\textwidth]{plots/RNG_Tc_normJ}
            }
            \subfigure[][]
            {
                \label{sfig:Tc_normJ:GG}
                \includegraphics[width=0.45\textwidth]{plots/GG_Tc_normJ}
            }

            \caption[Critical Temperature Normalized by Mean Sum of the Coupling Constants]
            {
                Top: Mean sum of the coupling constants to all
                neighbors over different disorder parameters for
                \subref{sfig:sumJ:RNG} the RNG and
                \subref{sfig:sumJ:GG} the GG.
                Bottom: Critical temperatures normalized by mean sum of the
                coupling constants \(\avg{\sum_{\avg{i,j}} J_{ij}}\) over different
                disorder parameters for
                \subref{sfig:Tc_normJ:RNG} the RNG and
                \subref{sfig:Tc_normJ:GG} the GG.
            }
            \label{fig:TcJ}
        \end{figure}\\
        Though Eq.\ \eqref{eq:exactHCTc} shows that there does not have
        to be a linear connection between \(J\) and \(T_c\), the best guess
        is a linear connection, because this model is derived from the
        square lattice, where the connection is linear. Therefore, one
        normalizes \(T_c\) with \(\avg{\sum_{\avg{i,j}} J_{ij}}\) as in
        Fig.\ \ref{fig:TcJ}\subref{sfig:Tc_normJ:RNG}\subref{sfig:Tc_normJ:GG}, the
        jump on the GG arises again, but \(T_c\) is now monotonically
        decreasing with increasing disorder parameter \(\sigma\).
        %~ This is a strong hint, that the change of the \(T_c\) with changing
        %~ \(\sigma\) is in some way connected to \(J\) and \(K\) which is an
        %~ expected conclusion.\\
        Moreover the forms of both curves are quite similar, but the
        one for the RNG in Fig.\ \ref{fig:TcJ}\subref{sfig:Tc_normJ:RNG}
        is generally lower and spans over a bigger temperature range than
        the curve of the GG in Fig.\ \ref{fig:TcJ}\subref{sfig:Tc_normJ:GG}.
        Both graph types have a plateau at \(0 < \sigma < 0.1\). The conclusion
        is that small disorder has little influence on this normalized critical
        temperature.
        Also both graph types exhibit a steep decline after the plateau before
        they approach an asymptotic limit for \(\sigma \gg 1\).\\
        If one looks at the behavior of \(\avg{\sum_{\avg{i,j}} J_{ij}}\)
        for \(\sigma \ll 1\) and consequently \(d_{ij} \approx 1\) the
        following approximation is valid.
        \begin{equation}
            \abs{1-d_{ij}} := \varepsilon \ll 1
        \end{equation}
        \begin{equation}
            J_{ij} = e^{\pm \alpha \varepsilon} \approx 1 \pm \varepsilon \mp \dots
        \end{equation}
        \begin{align}
            \avg{\sum_{\avg{i,j}} J_{ij}} &= \avg{\sum_{\avg{i,j}} (1 \pm \varepsilon)} \\
                                          &= \frac{1 \pm \varepsilon}{N} \sum_{\avg{i,j}} 1 \\
                                          &= K(1 \pm \varepsilon) \approx K
        \end{align}
        Therefore this normalization is independent of the choice of \(\alpha\)
        at small values of \(\sigma\). In Sec.\ \ref{appendix:fixedCoupling}
        in Fig.\ \ref{fig:Tc_deg_A0}\subref{sfig:Tc_norm_deg:RNG_A0}\subref{sfig:Tc_norm_deg:GG_A0}
        the values from Fig.\ \ref{fig:TcJ}\subref{sfig:Tc_normJ:RNG}\subref{sfig:Tc_normJ:GG}
        are compared to \(T / K\) for \(\alpha = 0\).

    \subsubsection{Course of the Critical Temperature $T_c$ with Fixed Coupling Constants $J$}
    \label{appendix:fixedCoupling}
        A quick analysis of this model with fixed coupling constants \(J = 1\)
        (i.e.\ \(\alpha=0\)) is performed. The results are displayed in Fig.\ \ref{fig:Tc_deg_A0}.
        The jump from \(\sigma=0\) to \(\sigma>0\) does not disappear as in Fig.\ \ref{fig:Tc_deg}
        for variable \(J\) with \(\alpha=0.5\). This suggests that the
        disappearance of the jump is a random special case for the function
        \(J_{ij}=e^{\alpha(1-d_{ij})}\) at \(\alpha=0.5\).\\
        The simulations were carried out on \(L \in \{16,32,64\}\) lattices
        for a subset of the \(\sigma\) and \(T\) used in the previous simulation.
        Also note that the degree \(K\) is
        the same used in \ref{fig:Tc}\subref{sfig:deg:RNG}\subref{sfig:deg:GG}
        because it is obviously independent of \(J\).
        Further, note that for small values of \(\sigma < 0.3\), \(T_c / K\)
        obtained using the fixed coupling strength \(J=1\) coincides with
        \(T_c / \avg{\sum_{\avg{i,j}}J_{ij}}\) obtained using the distance
        dependent coupling strength, Eq.\ \ref{eq:coupling}.
        This can be expected from the approximate analytic statement presented
        in Sec.\ \ref{sssec:J}.
        \begin{figure}[htb]
            \centering
            \subfigure[][]
            {
                \label{sfig:Tc:RNG_A0}
                \includegraphics[width=0.45\textwidth]{plots/RNG_Tc_A0}
            }
            \subfigure[][]
            {
                \label{sfig:Tc:GG_A0}
                \includegraphics[width=0.45\textwidth]{plots/GG_Tc_A0}
            }

            \subfigure[][]
            {
                \label{sfig:Tc_norm_deg:RNG_A0}
                \includegraphics[width=0.45\textwidth]{plots/RNG_Tc_norm_deg_A0}
            }
            \subfigure[][]
            {
                \label{sfig:Tc_norm_deg:GG_A0}
                \includegraphics[width=0.45\textwidth]{plots/GG_Tc_norm_deg_A0}
            }

            \caption[Critical Temperature and Critical Temperature Normalized by Degree of the Graph for Fixed Coupling Constants $J=1$]
            {
                Top: Critical Temperature \(T_c\) of the graph over different
                disorder parameters \(\sigma\) with fixed coupling constants \(J=1\) for
                \subref{sfig:deg:RNG} the RNG and
                \subref{sfig:deg:GG} the GG.
                Bottom: Critical temperatures normalized by degree \(K\) over
                different disorder parameters \(\sigma\) with fixed coupling constants \(J=1\) for
                \subref{sfig:Tc_norm_deg:RNG} the RNG and
                \subref{sfig:Tc_norm_deg:GG} the GG. The values of \(T_c / K\)
                are compared to those of \(T_c /  \avg{\sum_{\avg{i,j}}J_{ij}}\)
                from Sec.\ \ref{sssec:J}.
            }
            \label{fig:Tc_deg_A0}
        \end{figure}\\
        Because Ref.\ \cite{Janke1994} gives for the DT with \(J=1\) a
        critical temperature of \(T_{c,DT} = 3.80\)\footnote{more precise: a value of \(\frac{1}{T_c} \approx 0.263\) is given},
        it can be compared to the obtained \(T_{c,GG} = 2.19\)
        and \(T_{c,RNG} = 1.13\) for \(\sigma = 1.2\), which should be very
        similar to a Poisson distributed set of nodes.
        Note that while for the graph ensembles the relation
        \begin{equation}
            DT \supseteq GG \supseteq RNG
        \end{equation}
        holds (see Sec. \ref{ssec:graphtypes}), the relation for the
        critical points on these graphs
        \begin{equation}
            T_{c,DT} \ge T_{c,GG} \ge T_{c,RNG}
        \end{equation}
        is also true.
        Also keep in mind that the relation \(T_{c,GG} \ge T_{c,RNG}\)
        was true for \(\alpha = 0.5\) in the preceding sections.
        This phenomenon is also known from percolation where the relation
        \(p_{c,DT} \ge p_{c,GG} \ge p_{c,RNG}\) holds for the percolation
        threshold \(p_c\) -- the critical point of the percolation problem.
        The fact that the subgraph hierachy can be translated to
        the sequence of \(p_c\) for the subgraphs, is known as the containment
        theorem \cite{fisher}.
        Possibly the containment theorem is also applicable on this kind of
        problem.

\subsection{Critical Value of the Binder Cumulant}
    The value of the Binder cumulant at the critical point \(g_c\)
    depends strongly on boundary conditions but only weakly on the precise lattice
    structure \cite{BinderValue}. For periodic boundary conditions on a
    square lattice it is \(g_c \approx 0.916\) according to \cite{BinderValue}
        \footnote{Note that \cite{BinderValue} uses another definition of
            the Binder cumulant, and has to be normalized by \(\frac{2}{3}\)
            to match the definition in this thesis.}.
    Because the analysis of Sec.\ \ref{ssec:binderIntersections}
    yields \(g_c\) anyway, it is easy to check the consistency and
    behavior of \(g_c\) in the geometrically disordered Ising model.
    The error bars are the standard error of the six values obtained
    through the intersections.\\
    \begin{figure}[htbp]
        \centering
        \subfigure[for a RNG][]
        {
            \label{sfig:TcG:RNG}
            \includegraphics[width=0.45\textwidth]{plots/RNG_TcG}
        }
        \subfigure[for a GG][]
        {
            \label{sfig:TcG:GG}
            \includegraphics[width=0.45\textwidth]{plots/GG_TcG}
        }
        \caption[Values of the Binder Cumulant at the Critical Point $g_c$]
        {
            Values of the Binder cumulant at the critical point \(g_c\)
            for
            \subref{sfig:TcG:RNG} a RNG and
            \subref{sfig:TcG:GG} a GG for different \(\sigma\).
            The dotted line is the reference value for square lattices
            with periodic boundary conditions \cite{BinderValue}, which
            corresponds to \(\sigma = 0\).
        }
        \label{fig:TcG}
    \end{figure}\\
    Considering both plots in Fig.\ \ref{fig:TcG}, \(g_c\) is for low
    \(\sigma\) obviously always bigger than the known value. Though the
    deviations are only very small. Perhaps this overestimation is caused
    by the cubic spline interpolation used to acquire these \(g_c\) values.
    Or this are again finite size effects which would disappear for
    larger system sizes.
    For bigger \(\sigma\) the uncertainty gets greater, but the values
    do only differ by a few percent, hence even the big disorder and
    definition of nearest neighbors via a proximity graph does not change
    the value of \(g_c\) much. Though it is mentioned in \cite{BinderValue} that the
    lattice structure has a minor effect on \(g_c\), the uncertainty of
    \(g_c\) is too large to observe this. For a more exact analysis, new
    Monte Carlo simulations at \(T_c\) would be needed. But this is
    beyond the scope of this thesis. Anyway, the results are the expected
    behavior, because no major deviations from the value of \(g_c\) at
    \(\sigma = 0\) occur at larger \(\sigma\). Within error bars the
    values are all in reasonable agreement with \(g_c\).
